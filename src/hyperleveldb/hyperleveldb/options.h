// copyright (c) 2011 the leveldb authors. all rights reserved.
// use of this source code is governed by a bsd-style license that can be
// found in the license file. see the authors file for names of contributors.

#ifndef storage_hyperleveldb_include_options_h_
#define storage_hyperleveldb_include_options_h_

#include <stddef.h>

namespace hyperleveldb {

class cache;
class comparator;
class env;
class filterpolicy;
class logger;
class snapshot;

// db contents are stored in a set of blocks, each of which holds a
// sequence of key,value pairs.  each block may be compressed before
// being stored in a file.  the following enum describes which
// compression method (if any) is used to compress a block.
enum compressiontype {
  // note: do not change the values of existing entries, as these are
  // part of the persistent format on disk.
  knocompression     = 0x0,
  ksnappycompression = 0x1
};

// options to control the behavior of a database (passed to db::open)
struct options {
  // -------------------
  // parameters that affect behavior

  // comparator used to define the order of keys in the table.
  // default: a comparator that uses lexicographic byte-wise ordering
  //
  // requires: the client must ensure that the comparator supplied
  // here has the same name and orders keys *exactly* the same as the
  // comparator provided to previous open calls on the same db.
  const comparator* comparator;

  // if true, the database will be created if it is missing.
  // default: false
  bool create_if_missing;

  // if true, an error is raised if the database already exists.
  // default: false
  bool error_if_exists;

  // if true, the implementation will do aggressive checking of the
  // data it is processing and will stop early if it detects any
  // errors.  this may have unforeseen ramifications: for example, a
  // corruption of one db entry may cause a large number of entries to
  // become unreadable or for the entire db to become unopenable.
  // default: false
  bool paranoid_checks;

  // use the specified object to interact with the environment,
  // e.g. to read/write files, schedule background work, etc.
  // default: env::default()
  env* env;

  // any internal progress/error information generated by the db will
  // be written to info_log if it is non-null, or to a file stored
  // in the same directory as the db contents if info_log is null.
  // default: null
  logger* info_log;

  // -------------------
  // parameters that affect performance

  // amount of data to build up in memory (backed by an unsorted log
  // on disk) before converting to a sorted on-disk file.
  //
  // larger values increase performance, especially during bulk loads.
  // up to two write buffers may be held in memory at the same time,
  // so you may wish to adjust this parameter to control memory usage.
  // also, a larger write buffer will result in a longer recovery time
  // the next time the database is opened.
  //
  // default: 4mb
  size_t write_buffer_size;

  // number of open files that can be used by the db.  you may need to
  // increase this if your database has a large working set (budget
  // one open file per 2mb of working set).
  //
  // default: 1000
  int max_open_files;

  // control over blocks (user data is stored in a set of blocks, and
  // a block is the unit of reading from disk).

  // if non-null, use the specified cache for blocks.
  // if null, leveldb will automatically create and use an 8mb internal cache.
  // default: null
  cache* block_cache;

  // approximate size of user data packed per block.  note that the
  // block size specified here corresponds to uncompressed data.  the
  // actual size of the unit read from disk may be smaller if
  // compression is enabled.  this parameter can be changed dynamically.
  //
  // default: 4k
  size_t block_size;

  // number of keys between restart points for delta encoding of keys.
  // this parameter can be changed dynamically.  most clients should
  // leave this parameter alone.
  //
  // default: 16
  int block_restart_interval;

  // compress blocks using the specified compression algorithm.  this
  // parameter can be changed dynamically.
  //
  // default: ksnappycompression, which gives lightweight but fast
  // compression.
  //
  // typical speeds of ksnappycompression on an intel(r) core(tm)2 2.4ghz:
  //    ~200-500mb/s compression
  //    ~400-800mb/s decompression
  // note that these speeds are significantly faster than most
  // persistent storage speeds, and therefore it is typically never
  // worth switching to knocompression.  even if the input data is
  // incompressible, the ksnappycompression implementation will
  // efficiently detect that and will switch to uncompressed mode.
  compressiontype compression;

  // if non-null, use the specified filter policy to reduce disk reads.
  // many applications will benefit from passing the result of
  // newbloomfilterpolicy() here.
  //
  // default: null
  const filterpolicy* filter_policy;

  // is the database used with the replay mechanism?  if yes, the lower bound on
  // values to compact is (somewhat) left up to the application; if no, then
  // leveldb functions as usual, and uses snapshots to determine the lower
  // bound.  hyperleveldb will always maintain the integrity of snapshots, so
  // the application merely has the option to hold data as if it's holding a
  // snapshot.  this just prevents compaction from grabbing data before the app
  // can get a snapshot.
  //
  // default: false/no.
  bool manual_garbage_collection;

  // create an options object with default values for all fields.
  options();
};

// options that control read operations
struct readoptions {
  // if true, all data read from underlying storage will be
  // verified against corresponding checksums.
  // default: false
  bool verify_checksums;

  // should the data read for this iteration be cached in memory?
  // callers may wish to set this field to false for bulk scans.
  // default: true
  bool fill_cache;

  // if "snapshot" is non-null, read as of the supplied snapshot
  // (which must belong to the db that is being read and which must
  // not have been released).  if "snapshot" is null, use an impliicit
  // snapshot of the state at the beginning of this read operation.
  // default: null
  const snapshot* snapshot;

  readoptions()
      : verify_checksums(false),
        fill_cache(true),
        snapshot(null) {
  }
};

// options that control write operations
struct writeoptions {
  // if true, the write will be flushed from the operating system
  // buffer cache (by calling writablefile::sync()) before the write
  // is considered complete.  if this flag is true, writes will be
  // slower.
  //
  // if this flag is false, and the machine crashes, some recent
  // writes may be lost.  note that if it is just the process that
  // crashes (i.e., the machine does not reboot), no writes will be
  // lost even if sync==false.
  //
  // in other words, a db write with sync==false has similar
  // crash semantics as the "write()" system call.  a db write
  // with sync==true has similar crash semantics to a "write()"
  // system call followed by "fsync()".
  //
  // default: false
  bool sync;

  writeoptions()
      : sync(false) {
  }
};

}  // namespace hyperleveldb

#endif  // storage_hyperleveldb_include_options_h_
